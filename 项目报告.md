# 1. 自然语言处理

自然语言处理( Natural Language Processing, NLP)是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。

自然语言处理的任务类型大概如下

<img src="images/屏幕截图 2024-10-09 115917.png" alt="屏幕截图 2024-10-09 115917" />

其中“类别”可以理解为是标签或者分类，而“序列”可以理解为是一段文本或者一个数组。简单概况NLP的任务就是从一种数据类型转换成另一种数据类型的过程，这与绝大多数的机器学习模型相同或者类似，所以掌握了NLP的技术栈就等于掌握了机器学习的技术栈。

# 2. 文本聚类

## 2.1 什么是文本聚类

文本聚类是自然语言处理的应用之一。文本聚类是将相似的文本归为同一类别的任务。与分类不同，文本聚类不需要预先确定类别，而是根据文本数据的相似度来自动将文本分为不同的类别。文本聚类是无监督学习的一种形式，可以用于数据挖掘、信息检索、文本分类等任务。

## 2.2 传统的文本聚类方法

<img src="images/屏幕截图 2024-10-09 192711.png" alt="屏幕截图 2024-10-09 192711" />

传统的文本聚类方法通常涉及以下步骤：

1.特征选择：选择合适的文本特征表示方式。

2.相似度度量：根据文本特征之间的相似度度量来计算文本之间的相似度。

3.聚类算法：根据文本之间的相似度将文本聚类成不同的类别。

常见的文本聚类算法包括K-Means、层次聚类、密度聚类等。

然而，传统的文本聚类方法存在一些问题。首先，特征选择需要手动进行，需要领域专家参与，这一过程非常耗时且容易受到主观因素的影响；其次，传统的相似度度量方法无法充分捕捉文本之间的语义信息，因此在处理语义相似但表现形式不同的文本时会出现困难；最后，传统的聚类算法容易陷入局部最优解，而且聚类效果往往难以控制。

## 2.3 基于深度学习的文本聚类方法

<img src="images/屏幕截图 2024-10-09 192732.png" alt="屏幕截图 2024-10-09 192732" />

随着深度学习技术的发展，越来越多的研究者开始使用深度学习方法来解决文本聚类问题。基于深度学习的文本聚类方法可以概括为以下步骤：

1.文本表示：使用深度神经网络对文本进行表示学习，将文本映射到低维向量空间中。

2.相似度计算：计算不同文本在低维向量空间中的相似度。

3.聚类算法：根据相似度将文本聚类成不同的类别。

# 3. 对一个投诉数据集进行传统的文本聚类

## 3.1 下载Python以及配置环境

由于步骤过于繁琐，本篇不在此赘述，可上网查询

## 3.2 准备工作

打开Kaggle的Datasets界面并搜索，寻找一个有关学生投诉的数据集

<img src="images/屏幕截图 2024-10-09 152112.png" alt="屏幕截图 2024-10-09 152112.png" />

打开Kaggle的Code界面，新建Notebook

<img src="images/屏幕截图 2024-10-09 153040.png" alt="屏幕截图 2024-10-09 153040.png" />

点击Add Input，将刚刚浏览的数据集加入

<img src="images/屏幕截图 2024-10-09 153142.png" alt="屏幕截图 2024-10-09 153142.png" />

点击Upload，点击New Dataset，上传停用词列表(停用词自行拟定)

<img src="images/屏幕截图 2024-10-09 161305.png" alt="屏幕截图 2024-10-09 161305.png" />

上传成功后如下，在代码中直接调用其所在路径即可

<img src="images/屏幕截图 2024-10-09 161758.png" alt="屏幕截图 2024-10-09 161758.png" />

## 3.3 算法选择

以下是几种常见的聚类算法，由于所挑选的数据集已经将投诉类别给出，故选择最基础的K-Means聚类

### 3.3.1. K-means聚类

算法特点：

1.需要提前预知类的数量；

2.基于距离聚类。

优点：

速度快，计算简便

缺点：

K-means需要用初始随机种子点来启动优化，这个随机种子点太重要，不同随机种子点会得到完全不同的结果

### 3.3.2 均值漂移聚类

算法特点：

1.不需要预知类的数量；

2.基于滑动窗口，来找到数据点的密集区域（基于质心的聚类）。

优点：

基于密度的算法相比k-means受均值影响小

缺点：

窗口半径r的选择可能是不重要的

### 3.3.3 DBSCAN聚类

算法特点：

1.不需要预知类的数量；

2.先任选一个核心对象为“种子”，再由此出发确定对应的聚类簇。

缺点：

需要缺点距离r和minPoints

### 3.3.4 高斯混合聚类

算法特点：

1.需要预知类的数量；

2.假设数据点是呈高斯分布的；

3.用两个参数来描述

优点：

1.GMMs使用均值和标准差，簇可以呈现出椭圆形而不仅仅限制于圆形；k-means是GMMs的一个特殊情况，是方差在所有维度上都接近于0。

2.GMMs使用概率，所以一个数据点可以属于多个类，就是说混合聚类。

### 3.3.5 层次聚类

算法特点：

1.不需要预知类的数量；

2.试图在不同层次对数据集进行划分，从而形成树形的聚类结构;

3.可采用“自底向上”，“自顶向下”的拆分策略；

优点：

1.对于距离度量标准的选择不敏感;

2.当底层数据具有层次结构时，可以恢复层次结构。

缺点：

效率低，具有O(n^3)的时间复杂度

## 3.4 处理文本

将文本里面无意义的词语删除防止对结果造成影响

```bash
import pandas as pd
import re

def process_multiple_texts(texts):
    processed_texts = []
    for text in texts:
        train_seg_text = text.split()
        # 加载停用词
        stop_words_path = "/kaggle/input/stop-word/stopwords.txt"
        def get_stop_words():
            return set([item.strip() for item in open(stop_words_path, 'r').readlines()])
        stopwords = get_stop_words()

        # 去掉文本中的停用词
        train_st_text = [word for word in train_seg_text if word not in stopwords]

        processed_texts.append(' '.join(train_st_text))
    return processed_texts

# 读取 CSV 文件
data = pd.read_csv('/kaggle/input/university-students-complaints-and-reports/Datasetprojpowerbi.csv')
reports = data['Reports']

# 处理文本
result = process_multiple_texts(reports)
```
## 3.5 文本向量化

文本的向量化表示采用三种方式：使用 IDF 权重的哈希向量化表示、不使用 IDF 权重的哈希向量化表示以及 TFIDF 向量化表示，在做 hash 处理的时候，特征数设定为 1500，TFIDF 中使用全部词量

```bash
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
# Perform an IDF normalization on the output of HashingVectorizer
hasher = HashingVectorizer(n_features=1500, alternate_sign=False, norm=None)

vectorizer_hash_idf = make_pipeline(hasher, TfidfTransformer())

vectorizer_hash = HashingVectorizer(n_features=1500, alternate_sign=False, norm='l2')

vectorizer_tfidf = TfidfVectorizer(max_df=0.5, min_df=2, use_idf=True)

X_hash_idf = vectorizer_hash_idf.fit_transform(result)
X_hash = vectorizer_hash.fit_transform(result)
X_tfidf = vectorizer_tfidf.fit_transform(result)
```

对于数据维度过高，我们可以使用一些维度压缩技术进行处理，此处我们使用的是 SVD 及正则化处理，此处的输出维度我们限定到 40，如果压缩的目的是为了可视化，那么最常见的是压缩到 2 维

```bash
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer

print("Performing dimensionality reduction using LSA")
# Vectorizer results are normalized, which makes KMeans behave as
# spherical k-means for better results. Since LSA/SVD results are
# not normalized, we have to redo the normalization.
svd = TruncatedSVD(40)
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd, normalizer)

X_hash_idf_lsa = lsa.fit_transform(X_hash_idf)
X_hash_lsa = lsa.fit_transform(X_hash)
X_tfidf_lsa = lsa.fit_transform(X_tfidf)
```
## 3.6 文本聚类

此处使用两种算法，普通的 KMeans 算法以及同类的更具扩展性的 MiniBatchKMeans 算法，我们将两种算法分别应用于上面 6 种不同的数据处理结果上，共 12 组测试

```bash
from sklearn.cluster import MiniBatchKMeans
from sklearn.cluster import KMeans
minikm_X_hash_idf_lsa = MiniBatchKMeans(n_clusters=11, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=False)
minikm_X_hash_idf_lsa.fit(X_hash_idf_lsa)
minikm_X_hash_lsa = MiniBatchKMeans(n_clusters=11, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=False)
minikm_X_hash_lsa.fit(X_hash_lsa)
minikm_X_tfidf_lsa = MiniBatchKMeans(n_clusters=11, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=False)
minikm_X_tfidf_lsa.fit(X_tfidf_lsa)

minikm_X_hash_idf = MiniBatchKMeans(n_clusters=11, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=False)
minikm_X_hash_idf.fit(X_hash_idf)
minikm_X_hash = MiniBatchKMeans(n_clusters=11, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=False)
minikm_X_hash.fit(X_hash)
minikm_X_tfidf = MiniBatchKMeans(n_clusters=11, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=False)
minikm_X_tfidf.fit(X_tfidf)

km = KMeans(n_clusters=11, init='k-means++', max_iter=100, n_init=1, verbose=False)
km_X_hash_idf_lsa = km.fit(X_hash_idf_lsa)
km = KMeans(n_clusters=11, init='k-means++', max_iter=100, n_init=1, verbose=False)
km_X_hash_lsa = km.fit(X_hash_lsa)
km = KMeans(n_clusters=11, init='k-means++', max_iter=100, n_init=1, verbose=False)
km_X_tfidf_lsa = km.fit(X_tfidf_lsa)

km = KMeans(n_clusters=11, init='k-means++', max_iter=100, n_init=1, verbose=False)
km_X_hash_idf = km.fit(X_hash_idf)
km = KMeans(n_clusters=11, init='k-means++', max_iter=100, n_init=1, verbose=False)
km_X_hash = km.fit(X_hash)
km = KMeans(n_clusters=11, init='k-means++', max_iter=100, n_init=1, verbose=False)
km_X_tfidf = km.fit(X_tfidf)
```

下面我们结合数据集的真实标签及聚类结果标签，给出一些聚类算法中比较常见的评估指标

Homogeneity（同质性）：衡量同一类样本被划分到同一聚类中的程度

Completeness（完整性）：衡量所有同一类别的样本是否被划分到同一聚类中的程度

V-measure：综合考虑同质性和完整性的指标

Adjusted Rand Index（调整后的兰德指数）：用于衡量聚类结果与真实标签之间的一致性

Silhouette Coefficient（轮廓系数）：用于评估聚类的紧密度和分离度

```bash
from sklearn import metrics
import pandas as pd

df = pd.read_csv('/kaggle/input/university-students-complaints-and-reports/Datasetprojpowerbi.csv')
labels = df['Genre']

Xs = [X_hash_idf_lsa, X_hash_lsa, X_tfidf_lsa, 
     X_hash_idf, X_hash, X_tfidf, 
     X_hash_idf_lsa, X_hash_lsa, X_tfidf_lsa,
     X_hash_idf, X_hash, X_tfidf]

for index, method in enumerate([minikm_X_hash_idf_lsa, minikm_X_hash_lsa, minikm_X_tfidf_lsa,
          minikm_X_hash_idf, minikm_X_hash, minikm_X_tfidf,
          km_X_hash_idf_lsa, km_X_hash_lsa, km_X_tfidf_lsa,
          km_X_hash_idf, km_X_hash, km_X_tfidf]):
    X = Xs[index]
    print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, method.labels_))
    print("Completeness: %0.3f" % metrics.completeness_score(labels, method.labels_))
    print("V-measure: %0.3f" % metrics.v_measure_score(labels, method.labels_))
    print("Adjusted Rand-Index: %.3f"
          % metrics.adjusted_rand_score(labels, method.labels_))
    print("Silhouette Coefficient: %0.3f"
          % metrics.silhouette_score(X, method.labels_, metric='euclidean'))
    # print("Calinski-Harabaz Index: %0.3f"
    #       % metrics.calinski_harabasz_score(X.toarray(), km.labels_))     # toarray 内存直接搞死
    print("----------------------------------------------------------------")
```
我们得到如下结果

```bash
Homogeneity: 0.524
Completeness: 0.502
V-measure: 0.513
Adjusted Rand-Index: 0.383
Silhouette Coefficient: 0.093
----------------------------------------------------------------
Homogeneity: 0.389
Completeness: 0.384
V-measure: 0.387
Adjusted Rand-Index: 0.226
Silhouette Coefficient: 0.095
----------------------------------------------------------------
Homogeneity: 0.615
Completeness: 0.597
V-measure: 0.605
Adjusted Rand-Index: 0.429
Silhouette Coefficient: 0.122
----------------------------------------------------------------
Homogeneity: 0.416
Completeness: 0.413
V-measure: 0.415
Adjusted Rand-Index: 0.297
Silhouette Coefficient: 0.022
----------------------------------------------------------------
Homogeneity: 0.415
Completeness: 0.403
V-measure: 0.409
Adjusted Rand-Index: 0.212
Silhouette Coefficient: 0.033
----------------------------------------------------------------
Homogeneity: 0.493
Completeness: 0.492
V-measure: 0.493
Adjusted Rand-Index: 0.304
Silhouette Coefficient: 0.027
----------------------------------------------------------------
Homogeneity: 0.641
Completeness: 0.620
V-measure: 0.630
Adjusted Rand-Index: 0.477
Silhouette Coefficient: 0.111
----------------------------------------------------------------
Homogeneity: 0.426
Completeness: 0.415
V-measure: 0.421
Adjusted Rand-Index: 0.251
Silhouette Coefficient: 0.099
----------------------------------------------------------------
Homogeneity: 0.607
Completeness: 0.593
V-measure: 0.600
Adjusted Rand-Index: 0.393
Silhouette Coefficient: 0.128
----------------------------------------------------------------
Homogeneity: 0.476
Completeness: 0.476
V-measure: 0.476
Adjusted Rand-Index: 0.250
Silhouette Coefficient: 0.025
----------------------------------------------------------------
Homogeneity: 0.420
Completeness: 0.407
V-measure: 0.413
Adjusted Rand-Index: 0.291
Silhouette Coefficient: 0.037
----------------------------------------------------------------
Homogeneity: 0.490
Completeness: 0.523
V-measure: 0.506
Adjusted Rand-Index: 0.234
Silhouette Coefficient: 0.028
----------------------------------------------------------------
```

3.7 结果展示

参考上述结果，我们最终选择km-tfidf-lsa组别。我们可以展示其聚类后的关键词

```bash
# km-tfidf-lsa
print("Top terms per cluster:")
original_space_centroids = svd.inverse_transform(minikm_X_tfidf_lsa.cluster_centers_)
order_centroids = original_space_centroids.argsort()[:, ::-1]
terms = vectorizer_tfidf.get_feature_names_out()

for i in range(11):
    print("Cluster %d:" % i, end='')
    for ind in order_centroids[i, :11]:
        print(' %s' % terms[ind], end='')
    print()
```
结果如下

```bash
Top terms per cluster:
Cluster 0: time the ve it hard transportation material class afford study athletic
Cluster 1: job opportunities students university the field career internships internship experience offer
Cluster 2: sports university programs the athletes it gender lack team culture athletic
Cluster 3: the food cafeteria offer options cantine overpriced campus quality stale sandwiches
Cluster 4: financial aid scholarships meet the pay process lot tuition students received
Cluster 5: students it international feel language cultural university opportunities hard learn campus
Cluster 6: academic workload advisors responsibilities limited access time causing balancing stress commitments
Cluster 7: options campus affordable the housing students university provide food it available
Cluster 8: student office affairs certificate certificates ve the time loan it processed
Cluster 9: health mental care medical expenses insurance pay worried it afford ve
Cluster 10: online classes access limited materials difficult students databases the assignments resources
```
至此我们已经完成了对数据集的传统聚类分析

# 4. 对一个投诉数据集进行基于深度学习的聚类分析

## 4.1 准备工作

下载文本向量化和向量降维的库，导入其他所需要库

```bash
!pip install text2vec
!pip install umap-learn
import pandas as pd
import torch
from sklearn.cluster import MiniBatchKMeans
import pandas as pd
from text2vec import SentenceModel
from sklearn.metrics import silhouette_score
import numpy as np
import umap 
import matplotlib.pyplot as plt
```

## 4.2 读取文本

```bash
df = pd.read_csv("/kaggle/input/university-students-complaints-and-reports/Datasetprojpowerbi.csv", nrows=1010)
comment_df = df[["Reports"]]
comment_df.to_excel("data.xlsx", index=False, header=None)
def read_xlsx(data_path):
    print("正在读取源文档...")
    df = pd.read_excel(data_path, header=None)
    sentences = df[0].tolist()
    sentences = list(set([str(i).strip() for i in sentences]))
    return sentences
```

## 4.3 文本向量化

我们借助text2vec库中深度学习的模型对文本进行向量化并将其映射到低维空间

```bash
def text2vec(sentences):
    print("正在将文本转换为向量，请耐心等待...")
    model = SentenceModel("shibing624/text2vec-base-multilingual")
    embeddings = []
    for i in sentences:
        embedding = model.encode(i)
        embeddings.append(embedding)
    umap.UMAP().fit_transform(embeddings) # 降维
    embeddings_array = np.array(embeddings)
    x = torch.from_numpy(embeddings_array)
    return x
```

## 4.4 文本聚类

构建K-means聚类模型对文本进行聚类并将结果可视化

```bash
def clustering(data_path="data.xlsx", k=15):
    sentences = read_xlsx(data_path)
    x = text2vec(sentences)
    print("正在将向量进行聚类，请耐心等待...")
    # 创建 K-means 聚类模型
    mbk = MiniBatchKMeans(n_clusters=k)
    mbk = mbk.fit(x)
    cluster_ids_x = mbk.labels_
    output_df = pd.DataFrame()
    output_df["句子"] = sentences
    output_df["类别"] = cluster_ids_x
    output_df = output_df.sort_values(by="类别", ascending=False)
    output_df.to_excel("output.xlsx", index=False) 
    reduced_x = umap.UMAP(n_components=2).fit_transform(x)
    plt.scatter(reduced_x[:, 0], reduced_x[:, 1], c=cluster_ids_x, cmap='rainbow')
    plt.colorbar()
    plt.title('Clustering Visualization')
    plt.show()

    clustering(data_path="data.xlsx", k=11)
    output_df = pd.read_excel("output.xlsx")
    output_df
```

## 4.5 结果展示

<img src="images/屏幕截图 2024-10-09 224257.png" alt="屏幕截图 2024-10-09 224257" />

<img src="images/屏幕截图 2024-10-09 224320.png" alt="屏幕截图 2024-10-09 224320" />

至此，我们已经完成了对数据集的基于深度学习的聚类分析

# 5. 借助大语言模型(LLM)对数据集进行聚类分析

无论是传统聚类分析还是基于深度学习的聚类分析，对文本的聚类效果都不够优秀，故我们尝试借助Microsoft Azure平台上的大语言模型对文本进行聚类分析。

## 5.1 在Azure平台上建立资源区

注册完Azure账号后点击Azure OpenAI 或搜索AZure OpenAI

<img src="images/屏幕截图 2024-10-09 230730.png" alt="屏幕截图 2024-10-09 230730" />

<img src="images/屏幕截图 2024-10-09 231125.png" alt="屏幕截图 2024-10-09 231125" />

点击后再点击创建

<img src="images/屏幕截图 2024-10-09 231229.png" alt="屏幕截图 2024-10-09 231229" />

使用以下设置创建 Azure OpenAI 资源：

订阅：选择已获准访问 Azure OpenAI 服务的 Azure 订阅

资源组：选择或创建资源组

Region（区域）：澳大利亚东部/加拿大东部/美国东部/美国东部2/法国中部/日本东部/美国中北部/瑞典中部/瑞士北部/英国南部

名称：您选择的唯一名称

定价层：标准 S0

## 5.2 在Azure OpenAI Studio中部署AI

建立完资源区后点击资源区并转至Azure OpenAI Studio








