# 1. 自然语言处理

自然语言处理( Natural Language Processing, NLP)是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。

自然语言处理的任务类型大概如下

<img src="images/屏幕截图 2024-10-09 115917.png" alt="屏幕截图 2024-10-09 115917" />

其中“类别”可以理解为是标签或者分类，而“序列”可以理解为是一段文本或者一个数组。简单概况NLP的任务就是从一种数据类型转换成另一种数据类型的过程，这与绝大多数的机器学习模型相同或者类似，所以掌握了NLP的技术栈就等于掌握了机器学习的技术栈。

# 2. 文本聚类

## 2.1 什么是文本聚类

文本聚类是自然语言处理的应用之一。文本聚类是将相似的文本归为同一类别的任务。与分类不同，文本聚类不需要预先确定类别，而是根据文本数据的相似度来自动将文本分为不同的类别。文本聚类是无监督学习的一种形式，可以用于数据挖掘、信息检索、文本分类等任务。

## 2.2 传统的文本聚类方法

传统的文本聚类方法通常涉及以下步骤：

1.特征选择：选择合适的文本特征表示方式。

2.相似度度量：根据文本特征之间的相似度度量来计算文本之间的相似度。

3.聚类算法：根据文本之间的相似度将文本聚类成不同的类别。

常见的文本聚类算法包括K-Means、层次聚类、密度聚类等。

然而，传统的文本聚类方法存在一些问题。首先，特征选择需要手动进行，需要领域专家参与，这一过程非常耗时且容易受到主观因素的影响；其次，传统的相似度度量方法无法充分捕捉文本之间的语义信息，因此在处理语义相似但表现形式不同的文本时会出现困难；最后，传统的聚类算法容易陷入局部最优解，而且聚类效果往往难以控制。

## 2.3 基于深度学习的文本聚类方法

随着深度学习技术的发展，越来越多的研究者开始使用深度学习方法来解决文本聚类问题。基于深度学习的文本聚类方法可以概括为以下步骤：

1.文本表示：使用深度神经网络对文本进行表示学习，将文本映射到低维向量空间中。

2.相似度计算：计算不同文本在低维向量空间中的相似度。

3.聚类算法：根据相似度将文本聚类成不同的类别。

# 3. 对一个投诉数据集进行传统的文本聚类

## 3.1 下载Python以及配置环境

由于步骤过于繁琐，本篇不在此赘述，可上网查询

## 3.2 准备工作

打开Kaggle的Datasets界面并搜索，寻找一个有关学生投诉的数据集.

<img src="images/屏幕截图 2024-10-09 152112.png" alt="屏幕截图 2024-10-09 152112.png" />

打开Kaggle的Code界面，新建Notebook。

<img src="images/屏幕截图 2024-10-09 153040.png" alt="屏幕截图 2024-10-09 153040.png" />

点击Add Input，将刚刚浏览的数据集加入。

<img src="images/屏幕截图 2024-10-09 153142.png" alt="屏幕截图 2024-10-09 153142.png" />

点击Upload，点击New Dataset，上传停用词列表(停用词自行拟定)

<img src="images/屏幕截图 2024-10-09 161305.png" alt="屏幕截图 2024-10-09 161305.png" />

上传成功后如下，在代码中直接调用其所在路径即可。

<img src="images/屏幕截图 2024-10-09 161758.png" alt="屏幕截图 2024-10-09 161758.png" />

## 3.3 算法选择

以下是几种常见的聚类算法，由于所挑选的数据集已经将投诉类别给出，故选择最基础的K-Means聚类。

### 3.3.1. K-means聚类

算法特点：

1.需要提前预知类的数量；

2.基于距离聚类。

优点：

速度快，计算简便

缺点：

K-means需要用初始随机种子点来启动优化，这个随机种子点太重要，不同随机种子点会得到完全不同的结果

### 3.3.2 均值漂移聚类

算法特点：

1.不需要预知类的数量；

2.基于滑动窗口，来找到数据点的密集区域（基于质心的聚类）。

优点：

基于密度的算法相比k-means受均值影响小

缺点：

窗口半径r的选择可能是不重要的

### 3.3.3 DBSCAN聚类

算法特点：

1.不需要预知类的数量；

2.先任选一个核心对象为“种子”，再由此出发确定对应的聚类簇。

缺点：

需要缺点距离r和minPoints

### 3.3.4 高斯混合聚类

算法特点：

1.需要预知类的数量；

2.假设数据点是呈高斯分布的；

3.用两个参数来描述

优点：

1.GMMs使用均值和标准差，簇可以呈现出椭圆形而不仅仅限制于圆形；k-means是GMMs的一个特殊情况，是方差在所有维度上都接近于0。

2.GMMs使用概率，所以一个数据点可以属于多个类，就是说混合聚类。

### 3.3.5 层次聚类

算法特点：

1.不需要预知类的数量；

2.试图在不同层次对数据集进行划分，从而形成树形的聚类结构;

3.可采用“自底向上”，“自顶向下”的拆分策略；

优点：

1.对于距离度量标准的选择不敏感;

2.当底层数据具有层次结构时，可以恢复层次结构。

缺点：

效率低，具有O(n^3)的时间复杂度

## 3.4 处理文本

将文本里面无意义的词语删除防止对结果造成影响

```bash
import pandas as pd
import re

def process_multiple_texts(texts):
    processed_texts = []
    for text in texts:
        train_seg_text = text.split()
        # 加载停用词
        stop_words_path = "/kaggle/input/stop-word/stopwords.txt"
        def get_stop_words():
            return set([item.strip() for item in open(stop_words_path, 'r').readlines()])
        stopwords = get_stop_words()

        # 去掉文本中的停用词
        train_st_text = [word for word in train_seg_text if word not in stopwords]

        processed_texts.append(' '.join(train_st_text))
    return processed_texts

# 读取 CSV 文件
data = pd.read_csv('/kaggle/input/university-students-complaints-and-reports/Datasetprojpowerbi.csv')
reports = data['Reports']

# 处理文本
result = process_multiple_texts(reports)
```
## 3.5 文本向量化

文本的向量化表示采用三种方式：使用 IDF 权重的哈希向量化表示、不使用 IDF 权重的哈希向量化表示以及 TFIDF 向量化表示，在做 hash 处理的时候，特征数设定为 1500，TFIDF 中使用全部词量。

```bash
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
# Perform an IDF normalization on the output of HashingVectorizer
hasher = HashingVectorizer(n_features=1500, alternate_sign=False, norm=None)

vectorizer_hash_idf = make_pipeline(hasher, TfidfTransformer())

vectorizer_hash = HashingVectorizer(n_features=1500, alternate_sign=False, norm='l2')

vectorizer_tfidf = TfidfVectorizer(max_df=0.5, min_df=2, use_idf=True)

X_hash_idf = vectorizer_hash_idf.fit_transform(result)
X_hash = vectorizer_hash.fit_transform(result)
X_tfidf = vectorizer_tfidf.fit_transform(result)
```

对于数据维度过高，我们可以使用一些维度压缩技术进行处理，此处我们使用的是 SVD 及正则化处理，此处的输出维度我们限定到 40，如果压缩的目的是为了可视化，那么最常见的是压缩到 2 维。

```bash
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer

print("Performing dimensionality reduction using LSA")
# Vectorizer results are normalized, which makes KMeans behave as
# spherical k-means for better results. Since LSA/SVD results are
# not normalized, we have to redo the normalization.
svd = TruncatedSVD(40)
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd, normalizer)

X_hash_idf_lsa = lsa.fit_transform(X_hash_idf)
X_hash_lsa = lsa.fit_transform(X_hash)
X_tfidf_lsa = lsa.fit_transform(X_tfidf)
```

